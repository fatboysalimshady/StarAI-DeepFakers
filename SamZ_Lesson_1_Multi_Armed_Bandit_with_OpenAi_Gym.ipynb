{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SamZ-Lesson 1: Multi- Armed Bandit with OpenAi Gym.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/fatboysalimshady/StarAI-DeepFakers/blob/master/SamZ_Lesson_1_Multi_Armed_Bandit_with_OpenAi_Gym.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DzVfeL9n7W98",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "          _____                _____                    _____                    _____                    _____                    _____          \n",
        "         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        " /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        " \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "excxXVzFXgDf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson 1, exercise 1: The Multi Armed Bandit Problem with OpenAi Gym\n",
        "\n",
        "the purpose of this notebook is:\n",
        "\n",
        "\n",
        "\n",
        "1.   To understand the Gym Environment,\n",
        "2.   Implement Epsilon-Greedy on the bandit problem as discussed in the first lesson\n",
        "3.   **Win at slot machines!!.**\n",
        "\n",
        "Lets get started!\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VBwBIZPkZf_g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First we need to download & install the Gym Library so that it works in Colabs."
      ]
    },
    {
      "metadata": {
        "id": "MoRJN4PpXsGH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "on5JDYmWaK-w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Great!** now for our first bit of code\n"
      ]
    },
    {
      "metadata": {
        "id": "p8fBpXyfbk89",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets import the Gym class and walk through a basic example of Gym Code"
      ]
    },
    {
      "metadata": {
        "id": "5hK-jC9ceDuY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLePIoe7VJXS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gym's main purpose is to provide a large collection of \"environments\" that expose a common interface, using standardized inputs & outputs for Reinforcement Learning model testing purposes. You can find a listing of these environments below, as follows:"
      ]
    },
    {
      "metadata": {
        "id": "twrKYXVAVIp8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gym import envs\n",
        "print(envs.registry.all())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QA-998XefX85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unfortunatley, Gym does **not provide a bandit** environment so we need to import it, lets install one with the command below:"
      ]
    },
    {
      "metadata": {
        "id": "sGDaa_u8fjO3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JKCooper2/gym-bandits.git > /dev/null 2>&1\n",
        "!pip install ~/gym-bandits/. > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UX5HfOW7U3jS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install /content/gym-bandits/. > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kogFWTLPhcjx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And import the bandit library too"
      ]
    },
    {
      "metadata": {
        "id": "ecG0Xn0fhfd9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym_bandits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEkYlxG7cwjC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unlike in the lecture, where there was only 2 bandits & we were trying to figure out on average which one paid out the most, this time around we are going to be dealing with **TEN** (10) bandits!!!"
      ]
    },
    {
      "metadata": {
        "id": "nz924RhYdlif",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each bandit will have a payout with a normal distribution (bell curve), but the average payout or, centre of the distribution, will be different for each bandit, like in the image below"
      ]
    },
    {
      "metadata": {
        "id": "0NJTjv1xmi29",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://i.stack.imgur.com/SazYv.png)"
      ]
    },
    {
      "metadata": {
        "id": "rDhrNHQLU2xu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "."
      ]
    },
    {
      "metadata": {
        "id": "2e5lDZlheZuZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  >** >>>> It will be our goal to try & determine which bandit, out of the 10, pays out the most!!!!!!! <<<<**"
      ]
    },
    {
      "metadata": {
        "id": "HIFfNSFiU4NU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "."
      ]
    },
    {
      "metadata": {
        "id": "6GA0H5L3UNDm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets make a a variable called \"env\" to hold our freshly created 10 arm multi bandit environment"
      ]
    },
    {
      "metadata": {
        "id": "WJIh1OV_UMMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#gaussian distribution is just another name for \"normal distribution\" or bell curve (so many different names for the same thing!)\n",
        "env = gym.make('BanditTenArmedGaussian-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8E8LePgXTWF6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "."
      ]
    },
    {
      "metadata": {
        "id": "tHDprdS4BI1Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are now going to go over a basic example of how OpenAi Gym works."
      ]
    },
    {
      "metadata": {
        "id": "me9kN1MPWEEf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "run the code below & then we will explain what is going on here piece by piece"
      ]
    },
    {
      "metadata": {
        "id": "63v0tSoHbuwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "\n",
        "for i_episode in range(5):\n",
        "  \n",
        "    print(\"episode Number is\", i_episode)   \n",
        "    \n",
        "    action = env.action_space.sample() # sampling the \"action\" array which in this case only contains 10 \"options\" because there is 10 bandits\n",
        "        \n",
        "    print(\"action is\", action)\n",
        "        \n",
        "        \n",
        "    # here we taking the next \"step\" in our environment by taking in our action variable randomly selected above\n",
        "    observation, reward, done, info = env.step(action) \n",
        "        \n",
        "    print(\"observation space is: \",observation)\n",
        "    print(\"reward variable is: \",reward)\n",
        "    print(\"done flag is: \",done)\n",
        "    print(\"info variable is: \",info)\n",
        "        \n",
        "      \n",
        "            \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EHSzWhEmh1ha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "."
      ]
    },
    {
      "metadata": {
        "id": "MgJlSiCGbMBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reinforcement Learning** is an **extremely broad machine learning \"framework\"**, that looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "T3aynRR6ZcOI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "."
      ]
    },
    {
      "metadata": {
        "id": "HkYTLOA3aphJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![alt text](https://keon.io/images/deep-q-learning/rl.png)"
      ]
    },
    {
      "metadata": {
        "id": "HJmgkP_PjNsT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "explaining that picture above, The RL framework goes something like this:"
      ]
    },
    {
      "metadata": {
        "id": "PZnKxZw_jWlo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   You have an **AGENT** (machine learning algorithm), it the image above the agent is a human brain, lol\n",
        "2.   The agent takes an **ACTION**, in the image above available actions are using the joystick, up, down, left, right and the red button. So five actions available (5) in total.\n",
        "3.   a single ACTION is chosen (from our available five from the joystick) and fed to our **ENVIRONMENT**, which in our example above is the Atari game environment\n",
        "4.   at this point our ENVIRONMENT measures how good the action taken was and produces a **REWARD** signal. a Postive number is usually good & negative number is usually bad. \n",
        "5.   the environment then produces an **OBSERVATION**, again using our example above, think of the OBSERVATION as the next graphical \"frame\" of the game. In RL, we also call observations **STATES**  \n",
        "6.   The new OBSERVATION & REWARD signal (produced by the old observation-action pair) is then fed back to AGENT for it to decide what move to make next and so on and so forth\n"
      ]
    },
    {
      "metadata": {
        "id": "uH3nZ60oVUi8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, lets now take the above framework and see how it is implements on our example \"toy problem\" of the multi armed bandit"
      ]
    },
    {
      "metadata": {
        "id": "xuDzvJasVurw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Taking the code apart piece by piece"
      ]
    },
    {
      "metadata": {
        "id": "HjHOjKpsVymi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we have already created our enviroment with this line of code:\n",
        "\n",
        "\n",
        "```\n",
        "env = gym.make('BanditTenArmedGaussian-v0')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rDCrgRjLVndy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "but, we need to ask the environment to produce the first **OBSERVATION** (or as we also call it - state) so that we can feed it to our **AGENT** (the RL algorithm) to decide what to do next. \n"
      ]
    },
    {
      "metadata": {
        "id": "aKrvnNKkX1V6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So next up we get the *first* OBSERVATION by calling the following code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "observation = env.reset()\n",
        "\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "oxIQ6ZDEYL4N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next up, we create a 'for loop' that looped 5 times. In the Multi armed banded scenario an \"episode\" is just a single \"play\" of the game. So think of the For loop then as playing the multi armed bandit game 5 times.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for i_episode in range(5):\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CceslAxIZBst",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "each **ENVIRONMENT** is different. That is to say, each environment gives us different **ACTIONS** that are available, different **OBSERVATIONS** that are available etc"
      ]
    },
    {
      "metadata": {
        "id": "GFzJTTmFaTeu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the *10 armed bandit problem,* we should have *ten actions available *to us (as there should be 10 different slot machines, each with a different lever to pull)"
      ]
    },
    {
      "metadata": {
        "id": "IXRhRNz6aqxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can confirm this is the case by running the code below:"
      ]
    },
    {
      "metadata": {
        "id": "GMBt0CDuaqGs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VeGThbjxbje3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So taking this back to our example code previous, the next thing we do is take an action,* by randomly sampling from the* **ACTION SPACE**. Again, dont worry. The ACTION SPACE is just a number assigned to each our bandits. EG [0,1,2,3,4,5,6,7,8,9]. So for our example if we randomly sample the ACTION SPACE and get back the number 8 - all this means is that we will be pulling the \"lever\" on bandit 8."
      ]
    },
    {
      "metadata": {
        "id": "uM1J8_n4cbsT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is also important to note that at this point - we have not implemented any machine learning yet. We are only choosing actions at random. So lets choose one then. This was done with this line of code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "action = env.action_space.sample()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "H8v64MX8dKcU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have now completed step 2 of our Reinforcement Learning framework discussed earlier. Lets now do step 3, feeding the action into the environment, this was achieved with this line of code\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "observation, reward, done, info = env.step(action) \n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "r_j80CP2d3Va",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above line of code is actually doing a couple of things. We are feeding in our selected action with this line:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "env.step(action) \n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AwQ6JLupeHfY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And getting back 4 new variables in return , in this part of the code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "observation, reward, done, info = \n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JeAZp6rMeUfP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For now, we do not have to worry about the DONE and INFO variables. All we care about, in this tutorial is the OBSERVATION & REWARD variables. Also note that in this single line of code we have achieved step 4 & 5 of our RL framework discussed earlier in one go! "
      ]
    },
    {
      "metadata": {
        "id": "nAC11E-Jfl7a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Where we now breakaway from the RL framework, is that, we are not feeding the **REWARD** & **OBSERVATION** variables back to any **AGENT** (aka step 6 from the framework) to do anything intelligent yet. This is because we yet to create an **AGENT**!!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "S1pGRQ0Zk5na",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The final piece of the puzzle: the Reward-Averaging Sampling “learning rule”"
      ]
    },
    {
      "metadata": {
        "id": "Fcyht8exfgAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the lecture we learned about the epsilon-greedy agent. It is a synthesis of a purely exploratory agent and a completely greedy agent.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EHDa_E8TdLyd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the multiarmed bandit problem a **purely exploratory agent** would sample all the different bandits (or options) available uniformily - building up a distribution of each. However this has the downside such that the agent never gets to use its knowledge of the best option discovered so far. **A purely greedy agent  ** however would choose a bandit and stick to it's choice for all eternity - it will not try other agents to see if they provide better long term reward & can be thought of as being very \"narrow-minded\".\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tHEEPHKpfj9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To get the best of both worlds, the Epsilon-greedy agent is designed to ***explore*** at an ***Epsilon Chance*** (let say 10%) whilst the rest of the time (90%) it goes ***greedy*** on the best option it had discovered so far."
      ]
    },
    {
      "metadata": {
        "id": "q9EelHF8f0Fr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The idea here is that the Greedy mechanism helps the agent exploit the best option it has discovered so far, whilst the small amount of exploration leftover ensures that our agent keeps searching the other options available - to prove that there are not *even better options out there.*"
      ]
    },
    {
      "metadata": {
        "id": "ux6cmmhlh5js",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The last remaining piece of the puzzle is, how do we assign the idea of \"Value\", in the epsilon-greedy algorithm, to each of our bandits."
      ]
    },
    {
      "metadata": {
        "id": "F1n6kNFrxyMN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given some state S1 and an action A1, we can define a mathematical function to define the long term *expected* reward for this state action pair (state-action pairs are called \"Q values\", but we will get to this in a sec)"
      ]
    },
    {
      "metadata": {
        "id": "djdNT6huyjCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So the *expected* Q value (or average over time) for any one bandit (or choice), can be given by:"
      ]
    },
    {
      "metadata": {
        "id": "hl9CLLxP23NP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://image.ibb.co/evACsU/qforumula.png)"
      ]
    },
    {
      "metadata": {
        "id": "UpHON3EF3swc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "\n",
        "\n",
        "\n",
        "*  **Q** stands for **Quality**, this is an arbitary naming convention\n",
        "*  **n** stands for ***the number of times that bandit (or state) has been visited***\n",
        "* **r** is reward from visiting a bandit, n many times\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iOc9DRsV4rqf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So for example, lets say we visit bandit #1, 3 times. \n",
        "\n",
        "This means that we get three different rewards r1, r2 & r3. Because we visited bandit#1 3 times we also know that n = 3"
      ]
    },
    {
      "metadata": {
        "id": "BsoQUW0B5jXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Therefore: \n",
        "\n",
        "Q = (1/n)*(r1+r2+r3)"
      ]
    },
    {
      "metadata": {
        "id": "nbTr1MW66D2k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Q = 1/3*(r1+r2+r3)"
      ]
    },
    {
      "metadata": {
        "id": "8-1VDX-l6WbC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This whole process also has another name in life - we also call it , drumroll, \"the average\". However how the idea of average is used in the context here- is that we are only working out the average **for a single bandit**. \n",
        "\n",
        "In order to work out the average for another bandit, we will have to keep track of all of its reward variables & number of times it has been visited seperately"
      ]
    },
    {
      "metadata": {
        "id": "y6eUyp198V4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In a real life multi armed bandit scenario, we would have to keep track of many reward variables the longer our agent is run for. So in order to reduce the amount of computer memory that is in use, we can use some mathematical trickery to compress the above equation, such that we update our Q value with every visit to the bandit."
      ]
    },
    {
      "metadata": {
        "id": "LqZxc8wr85rs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "the formula then becomes:"
      ]
    },
    {
      "metadata": {
        "id": "oofNiNaj_Hu4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://image.ibb.co/nCPdhU/qforumulaupdated.png)"
      ]
    },
    {
      "metadata": {
        "id": "HxPT7op3ci17",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "this then is the heart of the epsilon greedy algorithm, or the forumula we use to update our Q table is pseudocode, **you have to use this code in the upcoming exercise to make the agent work.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hxNb0VgPc8yX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "NewQvalue = OldQvalue + ((reward - OldQvalue)/numberOfTimesLeverHasBeenPulledForThisBandit)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "b-VohcmEjx2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: creating your first (very simple) Agent"
      ]
    },
    {
      "metadata": {
        "id": "YRLF3R_Rj--G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are now ready to create your very first agent, the epsilon Greedy algorithm"
      ]
    },
    {
      "metadata": {
        "id": "2CsZVD-KkKmx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will need to keep track of which agent is the best, we do this by creating a big table, with 10 cells, one for each bandit. In computer terms this table is known as an array. We are creating this array to keep track of which bandit is doing the best for every time we play a game. For this exercise we will be using a powerful python array library known as *numpy*, so lets import that."
      ]
    },
    {
      "metadata": {
        "id": "1YbALrYCktJz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LuXsMBMTFosy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we also want to randomly initialise our enviroment, do this by running the code below:"
      ]
    },
    {
      "metadata": {
        "id": "5cc_prTsFyhP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#rerun this part of the code if you would like to \"reset\" or reinitialize your bandit environement\n",
        "env.seed(34)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i10ZSUORnzus",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let also make a variable that is the total number of bandits operating in our environment, **complete the code below:**"
      ]
    },
    {
      "metadata": {
        "id": "1ASjY3yNn8BG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numberOfBandits = 10 #???? hint we mentioned this number above"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hZTZsSATlJNw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we are going to call the array (remember this is just a table) which keeps track of which bandit is the best, a **Q TABLE**. "
      ]
    },
    {
      "metadata": {
        "id": "EFeQkqeTmW64",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Q in this case just stands for quality. The idea being that the number that is the highest in our table, is associated with the action we would like to take with the highest quality. but we are getting a little ahead of ourselves. If you did not get that dont worry. It will all become apparent in a couple of lines of reading!"
      ]
    },
    {
      "metadata": {
        "id": "iMBmGTs8nZ3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we also want to initialize our table so that all of the values are zero at the start, so lets do that"
      ]
    },
    {
      "metadata": {
        "id": "Zl4fw0YRnm2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q_table = np.zeros(numberOfBandits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wadn4RHooEz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Remember, we would like to keep track of the bandit with the *HIGHEST AVERAGE PAYOUT*. In order to do this we need another table to keep track of the number of times each bandit has been \"pulled\". We also want this table to be fulled with ones, this is to stop a divide by zero error later. **complete the code below:**"
      ]
    },
    {
      "metadata": {
        "id": "0VfVlrZEoWdi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_table = np.ones(numberOfBandits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vgYR_56KpJ-_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the lecture we talked a lot about epsilon. Lets create a new variable called \"epsilon\" and initialize it to 0.9. **complete the code below:**"
      ]
    },
    {
      "metadata": {
        "id": "yLi2bsWCpf65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epsilon = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4cT-7cPtFQj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below is the pseudo code for the epsilon greedy algorithm. In this implementation we are not going to vary epsilon - it is going to be a fixed number. \n",
        "\n",
        "This means that we will keep our exploration rate fixed."
      ]
    },
    {
      "metadata": {
        "id": "7OaxMbsU37Sc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now give you the pseudo code (code recipe) on how to implement the epsilon greedy algorithm. Look below at the \"recipe\". If you need help, all the ingredients on how to implement each part are below in the pseudo code."
      ]
    },
    {
      "metadata": {
        "id": "BaKRJx3Ft8US",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "create a for loop, to loop 1000 times\n",
        "\n",
        "      (IF STATEMENT, inside the loop,) generate a random number between 0 and 1 , if this number is less than \n",
        "      Epsilon enter \"exploitation mode\" aka use the best bandit we have discovered so far\n",
        "      \n",
        "            (inside the if statement) get the POSITION (index) in our array of the current max value within our\n",
        "            table, this index is the bandit that is giving the best payout so far.\n",
        "            \n",
        "            (inside the if statement) set your ACTION variable equal to the index we discovered in the last \n",
        "            statement\n",
        "            \n",
        "      (ELSE, otherwise..) if the number is greater than or equal to Epsilon, go into \"exploration mode\" and\n",
        "      choose a bandit at random \n",
        "      \n",
        "          \n",
        "            (inside the if statement) generate a random number between 0 and THE_NUMBER_OF_BANDITS\n",
        "            \n",
        "            (inside the if statement) set your ACTION variable to equal to the random number we just generated.\n",
        "            \n",
        "        \n",
        "            \n",
        "      (inside the loop) feed our ACTION variable into our environment by updating it with a step generated \n",
        "      by either of the steps above\n",
        "      \n",
        "      (inside the loop) now that we have gained some new information from our environment we want to update our \n",
        "      Q_table. We do this using the formula: Q_n+1 = Q_n + (R - Q_n)/n or in simpler english:\n",
        "      \n",
        "      NewQvalue = OldQvalue + ((reward - OldQvalue)/numberOfTimesLeverHasBeenPulledForThisBandit)\n",
        "      \n",
        "      Lets think about the intituition of what this forumula is doing. Implement the formula in code.\n",
        "      \n",
        "      \n",
        "      (inside the loop) now that we have updated our Q table, we also need to update the table that is keeping\n",
        "      track of how many times each bandit's lever has been pulled. Do this by adding +1 in the position\n",
        "      of our currently selected bandit in the N_TABLE array\n",
        "      \n",
        "      \n",
        "(OUTSIDE the loop) once everything is done, we would like to print the Bandit with the highest score! Using\n",
        "a print statement, and numpy's argmax function, using our Q table, print the bandit with the highest\n",
        "AVERAGE payout\n",
        "      \n",
        "   \n",
        "   \n",
        "  \n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4hqS1uXaUgX7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "."
      ]
    },
    {
      "metadata": {
        "id": "FnAfeNYWUBod",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Here are all the pieces required to build the above:**"
      ]
    },
    {
      "metadata": {
        "id": "vBbCYa3QUhrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to make a loop that loops a 10 times:"
      ]
    },
    {
      "metadata": {
        "id": "KKGvYZUmUl1S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#notice the indentation of the print statement, this indicates that this function (the print statement), is \"inside\" the loop\n",
        "\n",
        "for k in range(10):\n",
        "  print(\"now in loop iteration number: \",k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XAU8WC0bUzs-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to generate a random number between 0 & 1 using numpy:"
      ]
    },
    {
      "metadata": {
        "id": "E2jW4cMFU9xt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#rerun this multiple times to generate a different random number\n",
        "#remember np stands for \"numpy\" as declared above\n",
        "\n",
        "randomNumber = np.random.random(1)[0] \n",
        "\n",
        "print(\"random number is \", randomNumber)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n28zjNfQVXj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to create an if statement to see if our random number is less than epsilon"
      ]
    },
    {
      "metadata": {
        "id": "elf3xvsVWcbW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#epsilon should be set to 0.9 above...\n",
        "\n",
        "if(randomNumber < epsilon):\n",
        "  print(\"doing something less than epsilon\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ka0VSYXWu8l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to create an if statement to see if our random number is less than epsilon and do something else if this is not the case"
      ]
    },
    {
      "metadata": {
        "id": "ab6kK3F0Ws3J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(randomNumber < epsilon):\n",
        "  print(\"doing something less than epsilon\")\n",
        "else:\n",
        "  print(\"doing something more than epsilon\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYAlpVatXhwL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to lookup the best bandit's *location* (index) in our Q table, again remember there are 10 values to choose from (10 bandits) & we need to look up the index (location) in our Q_table array of the bandit with the highest score. Numpy's argmax() function allows us to do this.\n"
      ]
    },
    {
      "metadata": {
        "id": "rXuQ87hkXlpS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_bandit = np.argmax(q_table)\n",
        "\n",
        "print(\"best bandit is \",best_bandit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxbGaOG_YROy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "how to assign the index of our best action so that OpenAi gym can understand it:"
      ]
    },
    {
      "metadata": {
        "id": "IzD6aQHRYatW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "action = best_bandit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BYE8HoMPYmHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "how to generate a random bandit ACTION number between 0 to 9 to take:"
      ]
    },
    {
      "metadata": {
        "id": "nds0qfLCY3d4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#run this multiple times to see different results\n",
        "random_number = np.random.random_integers(numberOfBandits-1)\n",
        "\n",
        "print(random_number)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4A6eKeMfY7MG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "how to update OpenAi gym one time step into the future"
      ]
    },
    {
      "metadata": {
        "id": "vRPq3EN-ZB08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "observation, reward, done, info = env.step(action) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-PQn_PFOZP_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to directly access a numpy array's value, in this example we are accessing the best bandit's so far payout"
      ]
    },
    {
      "metadata": {
        "id": "JQ8CQpAlZVX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_table = np.array([2,3,1,0,5,7,9,8,6,4])\n",
        "\n",
        "example_table[best_bandit]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YvrXwE05fZMc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To print out all the values of a numpy array for testing purposes, use this code"
      ]
    },
    {
      "metadata": {
        "id": "eiyUZQggff5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(example_table[:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ie_RqC6LdCAQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "after looping 1000 times we would like to present our result - the best bandit out of the 10. Using Numpys argmax function:"
      ]
    },
    {
      "metadata": {
        "id": "mvp8I-XBdfKv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# think about the result after running this relative to our example table above:\n",
        "\n",
        "print('and the best bandit is....', np.argmax(example_table))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9KqH2ScdvwI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code up Epsilon-greedy below"
      ]
    },
    {
      "metadata": {
        "id": "kmH313czeI0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now have everything required to code up epsilon greedy. Using our **recipe**, and all the **blocks above,** code up an implementation of epsilon greedy below:"
      ]
    },
    {
      "metadata": {
        "id": "LAIuudWtc-Sx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#observation = env.reset()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env.seed(34)\n",
        "\n",
        "numberOfBandits = 10\n",
        "q_table = np.zeros(numberOfBandits)\n",
        "n_table = np.ones(numberOfBandits)\n",
        "\n",
        "epsilon = 0.8\n",
        "\n",
        "for k in range(1000):\n",
        "  #print(\"now in loop iteration number: \",k)\n",
        "  randomNumber = np.random.random(1)[0]\n",
        "  #print(\"random number is \", randomNumber)\n",
        "  if(randomNumber < epsilon):\n",
        "    #print(\"doing something less than epsilon\")\n",
        "    best_bandit = np.argmax(q_table)\n",
        "    #print(\"best bandit is \",best_bandit)\n",
        "    action = best_bandit\n",
        "  else:\n",
        "    random_number = np.random.randint(numberOfBandits)\n",
        "    #print(random_number)\n",
        "    action = random_number\n",
        "  #print(action)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "  q_table[action] = q_table[action] + ((reward - q_table[action])/n_table[action])\n",
        "  n_table[action] =  n_table[action] + 1\n",
        "print('and the best bandit is....', np.argmax(q_table))\n",
        "\n",
        "print(q_table)\n",
        "print(n_table)\n",
        "  \n",
        "    \n",
        "actions = [0,1,2,3,4,5,6,7,8,9]\n",
        "plt.figure(1)\n",
        "plt.subplot(121)\n",
        "plt.bar(actions, n_table)\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Times action taken')\n",
        "plt.title('n_values')\n",
        "plt.grid(True)\n",
        "plt.xticks(actions)\n",
        "plt.subplot(122)\n",
        "plt.bar(actions, q_table)\n",
        "plt.xlabel('Actions')\n",
        "plt.ylabel('Values')\n",
        "plt.title('q_values')\n",
        "plt.grid(True)\n",
        "#plt.xticks(actions)\n",
        "plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nlKkzk-Psj4A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ssk4sxLSx5HV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5icjz-4Mzw8H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}